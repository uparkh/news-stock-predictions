{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Models Comparisons -- Reddit\n",
    "In this part (3), I'll be comparing the VADER pretrained model from NLTK's presets, and the FinBERT model from the PyTorch library.\n",
    "\n",
    "The idea is that using a sentiment analysis model trained on financial data will allow it to pick up financial terms and keywords from the corpora better than the general purpose NLTK pretrained sentiment model.\n",
    "\n",
    "I'm not sure exactly yet how I'll measure this effect, but for each model's results, I'll make a plot of sentiment results for each month of 2024. Then I'll decide from there.\n",
    "\n",
    "## Data Treatment\n",
    "The cleaned data consists of lemmatized top comments, and the \"headline\" column which has the post title + self text\n",
    "\n",
    "Here's what I'll do. For each post:\n",
    "- Get a sentiment score for the post title + self text (headline), call it $s_0$\n",
    "- Get a sentiment score for each top comment, call them $s_1, s_2, ..., s_{10}$\n",
    "- Get a weighted aggregate sentiment score for the post. Weighing a score $s_i$ less as $i$ increases. I will just begin with the simple function: $w(s_i) = \\frac{1}{100} * (i - 10)^2 + 0.01$ to multiply to a score to weigh it. The $0.01$ is to just avoid weighing the last element at 0.\n",
    "\n",
    "For each month, I will take the median score of the post scores of that month as the representation for the entire month, as this statistic is more resistant to outliers.\n",
    "\n",
    "I have to do it like this because sentiment analysis really starts to break when the text gets too long, either with NLTK's VADER, or with FinBERT. You will see this in my previous commits if you want to look, but essentially I tried to combine all the info associated with each post into one supertext of the post, and tried to run an analysis ont that, but the models failed spectacularly with incredibly large bodies of text like that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reddit-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>month</th>\n",
       "      <th>post_id</th>\n",
       "      <th>tc0</th>\n",
       "      <th>tc1</th>\n",
       "      <th>tc2</th>\n",
       "      <th>tc3</th>\n",
       "      <th>tc4</th>\n",
       "      <th>tc5</th>\n",
       "      <th>tc6</th>\n",
       "      <th>tc7</th>\n",
       "      <th>tc8</th>\n",
       "      <th>tc9</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>investing</td>\n",
       "      <td>Aug</td>\n",
       "      <td>1ev6ov8</td>\n",
       "      <td>mcd dividend stock big yearly return compare b...</td>\n",
       "      <td>comment totally wrong mcd trade nearly bb per ...</td>\n",
       "      <td>nobody buy hold</td>\n",
       "      <td>great point thanks</td>\n",
       "      <td>one word hamburglar</td>\n",
       "      <td>actual answer bid ask spread think basis c spr...</td>\n",
       "      <td>sit mine enjoy dividend</td>\n",
       "      <td>remove</td>\n",
       "      <td>maybe share price around large typical stock c...</td>\n",
       "      <td>sounds like good thing</td>\n",
       "      <td>mcdonald s stock big bidask spread delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>investing</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1e7f6g5</td>\n",
       "      <td>s day week dca etf</td>\n",
       "      <td>individual stock reason happy consumer service...</td>\n",
       "      <td>time market beat time market pick needle hayst...</td>\n",
       "      <td>everybody talk pe ratio already moon analysts ...</td>\n",
       "      <td>keep buying fskax every paycheck keep go every...</td>\n",
       "      <td>every week get pay every week get pay every we...</td>\n",
       "      <td>costco individual stock</td>\n",
       "      <td>solid business management customer adoption s ...</td>\n",
       "      <td>need scratch gamble itch fomo</td>\n",
       "      <td>nice discover undervalue</td>\n",
       "      <td>top reason buy buy stock get general understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Aug</td>\n",
       "      <td>1ema5ue</td>\n",
       "      <td>actual fuck read</td>\n",
       "      <td>remove</td>\n",
       "      <td>already x investment regard</td>\n",
       "      <td>gt go eventually wind position nt think far ahead</td>\n",
       "      <td>m unrealized gain op please sell m live dividend</td>\n",
       "      <td>either smart stupid thing ve ever read</td>\n",
       "      <td>spend lego even taste good</td>\n",
       "      <td>get m ct avg cost per ct yet total cost basis ...</td>\n",
       "      <td>remove</td>\n",
       "      <td>hank</td>\n",
       "      <td>update spend quarter million dollar rock previ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit month  post_id  \\\n",
       "4391       investing   Aug  1ev6ov8   \n",
       "4277       investing   Jul  1e7f6g5   \n",
       "1938  wallstreetbets   Aug  1ema5ue   \n",
       "\n",
       "                                                    tc0  \\\n",
       "4391  mcd dividend stock big yearly return compare b...   \n",
       "4277                                 s day week dca etf   \n",
       "1938                                   actual fuck read   \n",
       "\n",
       "                                                    tc1  \\\n",
       "4391  comment totally wrong mcd trade nearly bb per ...   \n",
       "4277  individual stock reason happy consumer service...   \n",
       "1938                                             remove   \n",
       "\n",
       "                                                    tc2  \\\n",
       "4391                                    nobody buy hold   \n",
       "4277  time market beat time market pick needle hayst...   \n",
       "1938                        already x investment regard   \n",
       "\n",
       "                                                    tc3  \\\n",
       "4391                                 great point thanks   \n",
       "4277  everybody talk pe ratio already moon analysts ...   \n",
       "1938  gt go eventually wind position nt think far ahead   \n",
       "\n",
       "                                                    tc4  \\\n",
       "4391                                one word hamburglar   \n",
       "4277  keep buying fskax every paycheck keep go every...   \n",
       "1938   m unrealized gain op please sell m live dividend   \n",
       "\n",
       "                                                    tc5  \\\n",
       "4391  actual answer bid ask spread think basis c spr...   \n",
       "4277  every week get pay every week get pay every we...   \n",
       "1938             either smart stupid thing ve ever read   \n",
       "\n",
       "                             tc6  \\\n",
       "4391     sit mine enjoy dividend   \n",
       "4277     costco individual stock   \n",
       "1938  spend lego even taste good   \n",
       "\n",
       "                                                    tc7  \\\n",
       "4391                                             remove   \n",
       "4277  solid business management customer adoption s ...   \n",
       "1938  get m ct avg cost per ct yet total cost basis ...   \n",
       "\n",
       "                                                    tc8  \\\n",
       "4391  maybe share price around large typical stock c...   \n",
       "4277                      need scratch gamble itch fomo   \n",
       "1938                                             remove   \n",
       "\n",
       "                           tc9  \\\n",
       "4391    sounds like good thing   \n",
       "4277  nice discover undervalue   \n",
       "1938                      hank   \n",
       "\n",
       "                                               headline  \n",
       "4391          mcdonald s stock big bidask spread delete  \n",
       "4277  top reason buy buy stock get general understan...  \n",
       "1938  update spend quarter million dollar rock previ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['post_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>month</th>\n",
       "      <th>tc0</th>\n",
       "      <th>tc1</th>\n",
       "      <th>tc2</th>\n",
       "      <th>tc3</th>\n",
       "      <th>tc4</th>\n",
       "      <th>tc5</th>\n",
       "      <th>tc6</th>\n",
       "      <th>tc7</th>\n",
       "      <th>tc8</th>\n",
       "      <th>tc9</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4605</th>\n",
       "      <td>investing</td>\n",
       "      <td>Nov</td>\n",
       "      <td>nvidia main topic thanksgiving diner guess tim...</td>\n",
       "      <td>tesla hardware year ahead nvidia elon spend bi...</td>\n",
       "      <td>say s crap storm room everyone cry</td>\n",
       "      <td>hear ya s consolation stock main thing talk ye...</td>\n",
       "      <td>nt want talk</td>\n",
       "      <td>summary sure nvda super specialize chip first ...</td>\n",
       "      <td>yes make sense give iterative sort chip making...</td>\n",
       "      <td>say cant happen engineer leave nvidia tesla cr...</td>\n",
       "      <td>work ml space close decade nvidia position wel...</td>\n",
       "      <td>heard first folk ai kill turkey thanksgiving t...</td>\n",
       "      <td>talking dad nvidia thanksgiving dad active inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>investing</td>\n",
       "      <td>Sep</td>\n",
       "      <td>yr treasury pay almost interest rate predict f...</td>\n",
       "      <td>get year tbill even need worry stocksetfs goal</td>\n",
       "      <td>almost everything pretax pre inflation point m...</td>\n",
       "      <td>money market fund pay right</td>\n",
       "      <td>nobody list return pre inflation except seem e...</td>\n",
       "      <td>trinity study use stocksbonds portfolio could ...</td>\n",
       "      <td>low risk specially op timeframe three year</td>\n",
       "      <td>would think investment expensive future compare</td>\n",
       "      <td>poster say put treasury take profit invest inv...</td>\n",
       "      <td>put m vt collect dividend earning k year divid...</td>\n",
       "      <td>st many questions yr passive income m m recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>finance</td>\n",
       "      <td>Dec</td>\n",
       "      <td>thats one way point</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyse close jan honor late former president jim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit month                                                tc0  \\\n",
       "4605  investing   Nov  nvidia main topic thanksgiving diner guess tim...   \n",
       "4479  investing   Sep  yr treasury pay almost interest rate predict f...   \n",
       "3532    finance   Dec                                thats one way point   \n",
       "\n",
       "                                                    tc1  \\\n",
       "4605  tesla hardware year ahead nvidia elon spend bi...   \n",
       "4479     get year tbill even need worry stocksetfs goal   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc2  \\\n",
       "4605                 say s crap storm room everyone cry   \n",
       "4479  almost everything pretax pre inflation point m...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc3  \\\n",
       "4605  hear ya s consolation stock main thing talk ye...   \n",
       "4479                        money market fund pay right   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc4  \\\n",
       "4605                                       nt want talk   \n",
       "4479  nobody list return pre inflation except seem e...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc5  \\\n",
       "4605  summary sure nvda super specialize chip first ...   \n",
       "4479  trinity study use stocksbonds portfolio could ...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc6  \\\n",
       "4605  yes make sense give iterative sort chip making...   \n",
       "4479         low risk specially op timeframe three year   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc7  \\\n",
       "4605  say cant happen engineer leave nvidia tesla cr...   \n",
       "4479    would think investment expensive future compare   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc8  \\\n",
       "4605  work ml space close decade nvidia position wel...   \n",
       "4479  poster say put treasury take profit invest inv...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc9  \\\n",
       "4605  heard first folk ai kill turkey thanksgiving t...   \n",
       "4479  put m vt collect dividend earning k year divid...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                               headline  \n",
       "4605  talking dad nvidia thanksgiving dad active inv...  \n",
       "4479  st many questions yr passive income m m recent...  \n",
       "3532  nyse close jan honor late former president jim...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cryptocurrency' 'wallstreetbets' 'finance' 'investing']\n",
      "['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n"
     ]
    }
   ],
   "source": [
    "subreddits = df['subreddit'].unique()\n",
    "print(subreddits)\n",
    "\n",
    "months = df['month'].unique()\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of 'headline': 251.26746131325805\n",
      "Average character length of 'tc0': 101.6719512195122\n",
      "Average character length of 'tc1': 104.11005502751375\n",
      "Average character length of 'tc2': 101.50442477876106\n",
      "Average character length of 'tc3': 98.6069779374038\n",
      "Average character length of 'tc4': 102.36372950819673\n",
      "Average character length of 'tc5': 98.59127291505293\n",
      "Average character length of 'tc6': 94.35177968303455\n",
      "Average character length of 'tc7': 95.81495960385718\n",
      "Average character length of 'tc8': 97.39321148825066\n",
      "Average character length of 'tc9': 94.92190775681341\n"
     ]
    }
   ],
   "source": [
    "# Average character length of the text in the combined dataframe\n",
    "cols = ['headline'] + [f'tc{i}' for i in range(10)]\n",
    "for col in cols:\n",
    "    avg_length = df[col].str.len().mean()\n",
    "    print(f\"Average character length of '{col}': {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns are not of the correct data type\n",
    "df[cols] = df[cols].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay these are all reasonable character lengths, so I think the sentiment analyses should be much nicer compared to my previous attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Analysis\n",
    "In this section I'll use NLTK's sentiment analysis to convert all the columns into sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sphere las vegas loses million three months million past year\n"
     ]
    }
   ],
   "source": [
    "sample_headline = df['headline'].sample(1).values[0]\n",
    "print(sample_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.204, 'neu': 0.796, 'pos': 0.0, 'compound': -0.3182}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(sample_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Scores\n",
    "So this mechanism divides it's output into a negative, neutral, positive, and compound score.\n",
    "I'll just use compound for now, and see what the scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis and create new columns for each score\n",
    "for col in cols:\n",
    "    df[f'nltk_{col}'] = df[col].apply(lambda x: pd.Series(sia.polarity_scores(x)['compound']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk_headline</th>\n",
       "      <th>nltk_tc0</th>\n",
       "      <th>nltk_tc1</th>\n",
       "      <th>nltk_tc2</th>\n",
       "      <th>nltk_tc3</th>\n",
       "      <th>nltk_tc4</th>\n",
       "      <th>nltk_tc5</th>\n",
       "      <th>nltk_tc6</th>\n",
       "      <th>nltk_tc7</th>\n",
       "      <th>nltk_tc8</th>\n",
       "      <th>nltk_tc9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.194423</td>\n",
       "      <td>0.103991</td>\n",
       "      <td>0.117702</td>\n",
       "      <td>0.115965</td>\n",
       "      <td>0.118734</td>\n",
       "      <td>0.117597</td>\n",
       "      <td>0.112533</td>\n",
       "      <td>0.110985</td>\n",
       "      <td>0.110592</td>\n",
       "      <td>0.117524</td>\n",
       "      <td>0.106035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.503610</td>\n",
       "      <td>0.423685</td>\n",
       "      <td>0.423804</td>\n",
       "      <td>0.418796</td>\n",
       "      <td>0.405617</td>\n",
       "      <td>0.406864</td>\n",
       "      <td>0.407285</td>\n",
       "      <td>0.403414</td>\n",
       "      <td>0.407999</td>\n",
       "      <td>0.405583</td>\n",
       "      <td>0.402490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.986300</td>\n",
       "      <td>-0.984000</td>\n",
       "      <td>-0.998500</td>\n",
       "      <td>-0.975000</td>\n",
       "      <td>-0.987600</td>\n",
       "      <td>-0.989000</td>\n",
       "      <td>-0.991900</td>\n",
       "      <td>-0.965100</td>\n",
       "      <td>-0.970900</td>\n",
       "      <td>-0.978300</td>\n",
       "      <td>-0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.402200</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.997700</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>0.990800</td>\n",
       "      <td>0.996400</td>\n",
       "      <td>0.997200</td>\n",
       "      <td>0.996600</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>0.994400</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nltk_headline     nltk_tc0     nltk_tc1     nltk_tc2     nltk_tc3  \\\n",
       "count    4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean        0.194423     0.103991     0.117702     0.115965     0.118734   \n",
       "std         0.503610     0.423685     0.423804     0.418796     0.405617   \n",
       "min        -0.986300    -0.984000    -0.998500    -0.975000    -0.987600   \n",
       "25%         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%         0.025800     0.000000     0.000000     0.000000     0.000000   \n",
       "75%         0.624900     0.401900     0.421500     0.421500     0.421500   \n",
       "max         0.997700     0.996100     0.996100     0.990800     0.996400   \n",
       "\n",
       "          nltk_tc4     nltk_tc5     nltk_tc6     nltk_tc7     nltk_tc8  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean      0.117597     0.112533     0.110985     0.110592     0.117524   \n",
       "std       0.406864     0.407285     0.403414     0.407999     0.405583   \n",
       "min      -0.989000    -0.991900    -0.965100    -0.970900    -0.978300   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.402200     0.421500     0.401900     0.401900     0.401900   \n",
       "max       0.997200     0.996600     0.999400     0.995800     0.994400   \n",
       "\n",
       "          nltk_tc9  \n",
       "count  4800.000000  \n",
       "mean      0.106035  \n",
       "std       0.402490  \n",
       "min      -0.985600  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.381800  \n",
       "max       0.993600  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_cols = ['nltk_' + col for col in cols]\n",
    "df[nltk_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so it seems that a majority of scores are slightly positive. But this is across the columns. Perhaps after I run my special weighted mean on each post and then take the median grouped by month, I'll see a different story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(row: pd.Series, cols: list[str]) -> float:\n",
    "    def w(i: int) -> float:\n",
    "        return (1/100) * (i - 10) ** 2\n",
    "\n",
    "    return sum([row[col] * w(i) for i, col in enumerate(cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nltk_mu'] = df.apply(lambda row: weighted_mean(row, nltk_cols), axis=1)\n",
    "nltk_month_avg = df.groupby('month', sort=False)['nltk_mu'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "Jan    0.606782\n",
       "Feb    0.574916\n",
       "Mar    0.469782\n",
       "Apr    0.480691\n",
       "May    0.596519\n",
       "Jun    0.523441\n",
       "Jul    0.514087\n",
       "Aug    0.426764\n",
       "Sep    0.465829\n",
       "Oct    0.493374\n",
       "Nov    0.566213\n",
       "Dec    0.480120\n",
       "Name: nltk_mu, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_month_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, still all positive! Note that this isn't a horrible thing, I shouldn't shape the results to my expectation. Looking at the actual S&P 500 graph:\n",
    "![S&P 500 Graph 2024](S&P_500_2024.png)\n",
    "\n",
    "Can see that there was a dip in March and July/August of that year, which does correspond to dips in sentiment scores around that time in the data, despite being positive, so perhaps there's something there! Let's standardize to see in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "Jan    1.567940\n",
       "Feb    1.014258\n",
       "Mar   -0.812498\n",
       "Apr   -0.622943\n",
       "May    1.389622\n",
       "Jun    0.119856\n",
       "Jul   -0.042674\n",
       "Aug   -1.559961\n",
       "Sep   -0.881187\n",
       "Oct   -0.402573\n",
       "Nov    0.863036\n",
       "Dec   -0.632876\n",
       "Name: nltk_mu, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_month_avgs_standardized = (nltk_month_avg - nltk_month_avg.mean()) / nltk_month_avg.std()\n",
    "nltk_month_avgs_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly pretty good! If we think out a monthly score of 0 representing a neutral attitude, a negative score seems to correlate to a bearish outlook versus a positive score being a bullish outlook in the short term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT Sentiment Analysis\n",
    "Let's see how FinBERT does, given that it is specially trained on financial texts and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "finance_sentiment = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9170143604278564},\n",
       " {'label': 'neutral', 'score': 0.9170143604278564}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_sentiment([sample_headline, sample_headline])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:03<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Trim length of all entries to 256 tokens\n",
    "# BERT tokens are defined differently to NLTK tokens, so I ran into issues with only accepting\n",
    "# 512 tokens. I decided to use 256 tokens instead to avoid this issue.\n",
    "def trim(text: str) -> str:\n",
    "    tokens = word_tokenize(text)\n",
    "    text = ' '.join(tokens[:256])\n",
    "    return text\n",
    "\n",
    "for col in tqdm(cols):\n",
    "    df[col] = df[col].apply(trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [12:47<00:00, 69.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Batch processing because we are interfacing with an ML model, faster than processing one by one\n",
    "for col in tqdm(cols):\n",
    "    df[f'finbert_{col}'] = finance_sentiment(df[col].tolist())['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finbert_headline</th>\n",
       "      <th>finbert_tc0</th>\n",
       "      <th>finbert_tc1</th>\n",
       "      <th>finbert_tc2</th>\n",
       "      <th>finbert_tc3</th>\n",
       "      <th>finbert_tc4</th>\n",
       "      <th>finbert_tc5</th>\n",
       "      <th>finbert_tc6</th>\n",
       "      <th>finbert_tc7</th>\n",
       "      <th>finbert_tc8</th>\n",
       "      <th>finbert_tc9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.833033</td>\n",
       "      <td>0.846037</td>\n",
       "      <td>0.842018</td>\n",
       "      <td>0.842846</td>\n",
       "      <td>0.846565</td>\n",
       "      <td>0.845241</td>\n",
       "      <td>0.846360</td>\n",
       "      <td>0.845257</td>\n",
       "      <td>0.848469</td>\n",
       "      <td>0.849284</td>\n",
       "      <td>0.849421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.118525</td>\n",
       "      <td>0.102189</td>\n",
       "      <td>0.106561</td>\n",
       "      <td>0.105406</td>\n",
       "      <td>0.101344</td>\n",
       "      <td>0.103228</td>\n",
       "      <td>0.100460</td>\n",
       "      <td>0.100523</td>\n",
       "      <td>0.097162</td>\n",
       "      <td>0.099106</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.339315</td>\n",
       "      <td>0.419712</td>\n",
       "      <td>0.360333</td>\n",
       "      <td>0.345769</td>\n",
       "      <td>0.377977</td>\n",
       "      <td>0.378054</td>\n",
       "      <td>0.381369</td>\n",
       "      <td>0.388704</td>\n",
       "      <td>0.354028</td>\n",
       "      <td>0.385005</td>\n",
       "      <td>0.376750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.794248</td>\n",
       "      <td>0.828548</td>\n",
       "      <td>0.821627</td>\n",
       "      <td>0.822934</td>\n",
       "      <td>0.834749</td>\n",
       "      <td>0.832425</td>\n",
       "      <td>0.831627</td>\n",
       "      <td>0.831258</td>\n",
       "      <td>0.835944</td>\n",
       "      <td>0.839440</td>\n",
       "      <td>0.842858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.881537</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "      <td>0.883306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.914856</td>\n",
       "      <td>0.910056</td>\n",
       "      <td>0.907943</td>\n",
       "      <td>0.906877</td>\n",
       "      <td>0.907400</td>\n",
       "      <td>0.906970</td>\n",
       "      <td>0.905709</td>\n",
       "      <td>0.904408</td>\n",
       "      <td>0.904562</td>\n",
       "      <td>0.908027</td>\n",
       "      <td>0.904848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.974264</td>\n",
       "      <td>0.963359</td>\n",
       "      <td>0.970647</td>\n",
       "      <td>0.962415</td>\n",
       "      <td>0.967508</td>\n",
       "      <td>0.967063</td>\n",
       "      <td>0.966618</td>\n",
       "      <td>0.969753</td>\n",
       "      <td>0.967875</td>\n",
       "      <td>0.960606</td>\n",
       "      <td>0.971689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       finbert_headline  finbert_tc0  finbert_tc1  finbert_tc2  finbert_tc3  \\\n",
       "count       4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean           0.833033     0.846037     0.842018     0.842846     0.846565   \n",
       "std            0.118525     0.102189     0.106561     0.105406     0.101344   \n",
       "min            0.339315     0.419712     0.360333     0.345769     0.377977   \n",
       "25%            0.794248     0.828548     0.821627     0.822934     0.834749   \n",
       "50%            0.881537     0.883306     0.883306     0.883306     0.883306   \n",
       "75%            0.914856     0.910056     0.907943     0.906877     0.907400   \n",
       "max            0.974264     0.963359     0.970647     0.962415     0.967508   \n",
       "\n",
       "       finbert_tc4  finbert_tc5  finbert_tc6  finbert_tc7  finbert_tc8  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean      0.845241     0.846360     0.845257     0.848469     0.849284   \n",
       "std       0.103228     0.100460     0.100523     0.097162     0.099106   \n",
       "min       0.378054     0.381369     0.388704     0.354028     0.385005   \n",
       "25%       0.832425     0.831627     0.831258     0.835944     0.839440   \n",
       "50%       0.883306     0.883306     0.883306     0.883306     0.883306   \n",
       "75%       0.906970     0.905709     0.904408     0.904562     0.908027   \n",
       "max       0.967063     0.966618     0.969753     0.967875     0.960606   \n",
       "\n",
       "       finbert_tc9  \n",
       "count  4800.000000  \n",
       "mean      0.849421  \n",
       "std       0.097800  \n",
       "min       0.376750  \n",
       "25%       0.842858  \n",
       "50%       0.883306  \n",
       "75%       0.904848  \n",
       "max       0.971689  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finbert_cols = ['finbert_' + col for col in cols]\n",
    "df[finbert_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['finbert_mu'] = df.apply(lambda row: weighted_mean(row, finbert_cols), axis=1)\n",
    "fb_month_avg = df.groupby('month', sort=False)['finbert_mu'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "Jan    3.254800\n",
       "Feb    3.253643\n",
       "Mar    3.235013\n",
       "Apr    3.238001\n",
       "May    3.238996\n",
       "Jun    3.215775\n",
       "Jul    3.250483\n",
       "Aug    3.230884\n",
       "Sep    3.245571\n",
       "Oct    3.246990\n",
       "Nov    3.237038\n",
       "Dec    3.235268\n",
       "Name: finbert_mu, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_month_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that all the scores still are quite high, but the subtle patterns are there that correlate to March and July/August dips. Maybe standardizing could be helpful to see the dips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_month_avgs_standardized = (fb_month_avg - fb_month_avg.mean()) / fb_month_avg.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk</th>\n",
       "      <th>finbert</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jan</th>\n",
       "      <td>1.567940</td>\n",
       "      <td>1.329582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb</th>\n",
       "      <td>1.014258</td>\n",
       "      <td>1.224191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar</th>\n",
       "      <td>-0.812498</td>\n",
       "      <td>-0.473030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apr</th>\n",
       "      <td>-0.622943</td>\n",
       "      <td>-0.200766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>1.389622</td>\n",
       "      <td>-0.110183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun</th>\n",
       "      <td>0.119856</td>\n",
       "      <td>-2.225564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jul</th>\n",
       "      <td>-0.042674</td>\n",
       "      <td>0.936322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aug</th>\n",
       "      <td>-1.559961</td>\n",
       "      <td>-0.849149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sep</th>\n",
       "      <td>-0.881187</td>\n",
       "      <td>0.488836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oct</th>\n",
       "      <td>-0.402573</td>\n",
       "      <td>0.618049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nov</th>\n",
       "      <td>0.863036</td>\n",
       "      <td>-0.288546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dec</th>\n",
       "      <td>-0.632876</td>\n",
       "      <td>-0.449743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nltk   finbert\n",
       "month                    \n",
       "Jan    1.567940  1.329582\n",
       "Feb    1.014258  1.224191\n",
       "Mar   -0.812498 -0.473030\n",
       "Apr   -0.622943 -0.200766\n",
       "May    1.389622 -0.110183\n",
       "Jun    0.119856 -2.225564\n",
       "Jul   -0.042674  0.936322\n",
       "Aug   -1.559961 -0.849149\n",
       "Sep   -0.881187  0.488836\n",
       "Oct   -0.402573  0.618049\n",
       "Nov    0.863036 -0.288546\n",
       "Dec   -0.632876 -0.449743"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([nltk_month_avgs_standardized, fb_month_avgs_standardized], axis=1, keys=['nltk', 'finbert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![S&P500 2024 Graph](S&P_500_2024.png)\n",
    "Little better, it seems that FinBERT follows similar trends to NLTK, but FinBERT's June is way too pessimistic, and FinBERT's July is way too optimistic. Based on the actual S&P500 graph, it seems that NLTK actually performed better, so I will be using NLTK for my plotting and analysis moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reddit-sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "410-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
