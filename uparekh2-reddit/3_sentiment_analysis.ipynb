{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Models Comparisons -- Reddit\n",
    "In this part (3), I'll be comparing the VADER pretrained model from NLTK's presets, and the FinBERT model from the PyTorch library.\n",
    "\n",
    "The idea is that using a sentiment analysis model trained on financial data will allow it to pick up financial terms and keywords from the corpora better than the general purpose NLTK pretrained sentiment model.\n",
    "\n",
    "I'm not sure exactly yet how I'll measure this effect, but for each model's results, I'll make a plot of sentiment results for each month of 2024. Then I'll decide from there.\n",
    "\n",
    "## Data Treatment\n",
    "The cleaned data consists of lemmatized top comments, and the \"headline\" column which has the post title + self text\n",
    "\n",
    "Here's what I'll do. For each post:\n",
    "- Get a sentiment score for the post title + self text (headline), call it $s_0$\n",
    "- Get a sentiment score for each top comment, call them $s_1, s_2, ..., s_{10}$\n",
    "- Get a weighted aggregate sentiment score for the post. Weighing a score $s_i$ less as $i$ increases. I will just begin with the simple function: $w(s_i) = \\frac{1}{100} * (i - 10)^2 + 0.01$ to multiply to a score to weigh it. The $0.01$ is to just avoid weighing the last element at 0.\n",
    "\n",
    "For each month, I will take the median score of the post scores of that month as the representation for the entire month, as this statistic is more resistant to outliers.\n",
    "\n",
    "I have to do it like this because sentiment analysis really starts to break when the text gets too long, either with NLTK's VADER, or with FinBERT. You will see this in my previous commits if you want to look, but essentially I tried to combine all the info associated with each post into one supertext of the post, and tried to run an analysis ont that, but the models failed spectacularly with incredibly large bodies of text like that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reddit-cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>month</th>\n",
       "      <th>post_id</th>\n",
       "      <th>tc0</th>\n",
       "      <th>tc1</th>\n",
       "      <th>tc2</th>\n",
       "      <th>tc3</th>\n",
       "      <th>tc4</th>\n",
       "      <th>tc5</th>\n",
       "      <th>tc6</th>\n",
       "      <th>tc7</th>\n",
       "      <th>tc8</th>\n",
       "      <th>tc9</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>investing</td>\n",
       "      <td>Aug</td>\n",
       "      <td>1ev6ov8</td>\n",
       "      <td>mcd dividend stock big yearly return compare b...</td>\n",
       "      <td>comment totally wrong mcd trade nearly bb per ...</td>\n",
       "      <td>nobody buy hold</td>\n",
       "      <td>great point thanks</td>\n",
       "      <td>one word hamburglar</td>\n",
       "      <td>actual answer bid ask spread think basis c spr...</td>\n",
       "      <td>sit mine enjoy dividend</td>\n",
       "      <td>remove</td>\n",
       "      <td>maybe share price around large typical stock c...</td>\n",
       "      <td>sounds like good thing</td>\n",
       "      <td>mcdonald s stock big bidask spread delete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>investing</td>\n",
       "      <td>Jul</td>\n",
       "      <td>1e7f6g5</td>\n",
       "      <td>s day week dca etf</td>\n",
       "      <td>individual stock reason happy consumer service...</td>\n",
       "      <td>time market beat time market pick needle hayst...</td>\n",
       "      <td>everybody talk pe ratio already moon analysts ...</td>\n",
       "      <td>keep buying fskax every paycheck keep go every...</td>\n",
       "      <td>every week get pay every week get pay every we...</td>\n",
       "      <td>costco individual stock</td>\n",
       "      <td>solid business management customer adoption s ...</td>\n",
       "      <td>need scratch gamble itch fomo</td>\n",
       "      <td>nice discover undervalue</td>\n",
       "      <td>top reason buy buy stock get general understan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Aug</td>\n",
       "      <td>1ema5ue</td>\n",
       "      <td>actual fuck read</td>\n",
       "      <td>remove</td>\n",
       "      <td>already x investment regard</td>\n",
       "      <td>gt go eventually wind position nt think far ahead</td>\n",
       "      <td>m unrealized gain op please sell m live dividend</td>\n",
       "      <td>either smart stupid thing ve ever read</td>\n",
       "      <td>spend lego even taste good</td>\n",
       "      <td>get m ct avg cost per ct yet total cost basis ...</td>\n",
       "      <td>remove</td>\n",
       "      <td>hank</td>\n",
       "      <td>update spend quarter million dollar rock previ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit month  post_id  \\\n",
       "4391       investing   Aug  1ev6ov8   \n",
       "4277       investing   Jul  1e7f6g5   \n",
       "1938  wallstreetbets   Aug  1ema5ue   \n",
       "\n",
       "                                                    tc0  \\\n",
       "4391  mcd dividend stock big yearly return compare b...   \n",
       "4277                                 s day week dca etf   \n",
       "1938                                   actual fuck read   \n",
       "\n",
       "                                                    tc1  \\\n",
       "4391  comment totally wrong mcd trade nearly bb per ...   \n",
       "4277  individual stock reason happy consumer service...   \n",
       "1938                                             remove   \n",
       "\n",
       "                                                    tc2  \\\n",
       "4391                                    nobody buy hold   \n",
       "4277  time market beat time market pick needle hayst...   \n",
       "1938                        already x investment regard   \n",
       "\n",
       "                                                    tc3  \\\n",
       "4391                                 great point thanks   \n",
       "4277  everybody talk pe ratio already moon analysts ...   \n",
       "1938  gt go eventually wind position nt think far ahead   \n",
       "\n",
       "                                                    tc4  \\\n",
       "4391                                one word hamburglar   \n",
       "4277  keep buying fskax every paycheck keep go every...   \n",
       "1938   m unrealized gain op please sell m live dividend   \n",
       "\n",
       "                                                    tc5  \\\n",
       "4391  actual answer bid ask spread think basis c spr...   \n",
       "4277  every week get pay every week get pay every we...   \n",
       "1938             either smart stupid thing ve ever read   \n",
       "\n",
       "                             tc6  \\\n",
       "4391     sit mine enjoy dividend   \n",
       "4277     costco individual stock   \n",
       "1938  spend lego even taste good   \n",
       "\n",
       "                                                    tc7  \\\n",
       "4391                                             remove   \n",
       "4277  solid business management customer adoption s ...   \n",
       "1938  get m ct avg cost per ct yet total cost basis ...   \n",
       "\n",
       "                                                    tc8  \\\n",
       "4391  maybe share price around large typical stock c...   \n",
       "4277                      need scratch gamble itch fomo   \n",
       "1938                                             remove   \n",
       "\n",
       "                           tc9  \\\n",
       "4391    sounds like good thing   \n",
       "4277  nice discover undervalue   \n",
       "1938                      hank   \n",
       "\n",
       "                                               headline  \n",
       "4391          mcdonald s stock big bidask spread delete  \n",
       "4277  top reason buy buy stock get general understan...  \n",
       "1938  update spend quarter million dollar rock previ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['post_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>month</th>\n",
       "      <th>tc0</th>\n",
       "      <th>tc1</th>\n",
       "      <th>tc2</th>\n",
       "      <th>tc3</th>\n",
       "      <th>tc4</th>\n",
       "      <th>tc5</th>\n",
       "      <th>tc6</th>\n",
       "      <th>tc7</th>\n",
       "      <th>tc8</th>\n",
       "      <th>tc9</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4605</th>\n",
       "      <td>investing</td>\n",
       "      <td>Nov</td>\n",
       "      <td>nvidia main topic thanksgiving diner guess tim...</td>\n",
       "      <td>tesla hardware year ahead nvidia elon spend bi...</td>\n",
       "      <td>say s crap storm room everyone cry</td>\n",
       "      <td>hear ya s consolation stock main thing talk ye...</td>\n",
       "      <td>nt want talk</td>\n",
       "      <td>summary sure nvda super specialize chip first ...</td>\n",
       "      <td>yes make sense give iterative sort chip making...</td>\n",
       "      <td>say cant happen engineer leave nvidia tesla cr...</td>\n",
       "      <td>work ml space close decade nvidia position wel...</td>\n",
       "      <td>heard first folk ai kill turkey thanksgiving t...</td>\n",
       "      <td>talking dad nvidia thanksgiving dad active inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4479</th>\n",
       "      <td>investing</td>\n",
       "      <td>Sep</td>\n",
       "      <td>yr treasury pay almost interest rate predict f...</td>\n",
       "      <td>get year tbill even need worry stocksetfs goal</td>\n",
       "      <td>almost everything pretax pre inflation point m...</td>\n",
       "      <td>money market fund pay right</td>\n",
       "      <td>nobody list return pre inflation except seem e...</td>\n",
       "      <td>trinity study use stocksbonds portfolio could ...</td>\n",
       "      <td>low risk specially op timeframe three year</td>\n",
       "      <td>would think investment expensive future compare</td>\n",
       "      <td>poster say put treasury take profit invest inv...</td>\n",
       "      <td>put m vt collect dividend earning k year divid...</td>\n",
       "      <td>st many questions yr passive income m m recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>finance</td>\n",
       "      <td>Dec</td>\n",
       "      <td>thats one way point</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nyse close jan honor late former president jim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit month                                                tc0  \\\n",
       "4605  investing   Nov  nvidia main topic thanksgiving diner guess tim...   \n",
       "4479  investing   Sep  yr treasury pay almost interest rate predict f...   \n",
       "3532    finance   Dec                                thats one way point   \n",
       "\n",
       "                                                    tc1  \\\n",
       "4605  tesla hardware year ahead nvidia elon spend bi...   \n",
       "4479     get year tbill even need worry stocksetfs goal   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc2  \\\n",
       "4605                 say s crap storm room everyone cry   \n",
       "4479  almost everything pretax pre inflation point m...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc3  \\\n",
       "4605  hear ya s consolation stock main thing talk ye...   \n",
       "4479                        money market fund pay right   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc4  \\\n",
       "4605                                       nt want talk   \n",
       "4479  nobody list return pre inflation except seem e...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc5  \\\n",
       "4605  summary sure nvda super specialize chip first ...   \n",
       "4479  trinity study use stocksbonds portfolio could ...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc6  \\\n",
       "4605  yes make sense give iterative sort chip making...   \n",
       "4479         low risk specially op timeframe three year   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc7  \\\n",
       "4605  say cant happen engineer leave nvidia tesla cr...   \n",
       "4479    would think investment expensive future compare   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc8  \\\n",
       "4605  work ml space close decade nvidia position wel...   \n",
       "4479  poster say put treasury take profit invest inv...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                                    tc9  \\\n",
       "4605  heard first folk ai kill turkey thanksgiving t...   \n",
       "4479  put m vt collect dividend earning k year divid...   \n",
       "3532                                                NaN   \n",
       "\n",
       "                                               headline  \n",
       "4605  talking dad nvidia thanksgiving dad active inv...  \n",
       "4479  st many questions yr passive income m m recent...  \n",
       "3532  nyse close jan honor late former president jim...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cryptocurrency' 'wallstreetbets' 'finance' 'investing']\n",
      "['Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep' 'Oct' 'Nov' 'Dec']\n"
     ]
    }
   ],
   "source": [
    "subreddits = df['subreddit'].unique()\n",
    "print(subreddits)\n",
    "\n",
    "months = df['month'].unique()\n",
    "print(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of 'headline': 251.26746131325805\n",
      "Average character length of 'tc0': 101.6719512195122\n",
      "Average character length of 'tc1': 104.11005502751375\n",
      "Average character length of 'tc2': 101.50442477876106\n",
      "Average character length of 'tc3': 98.6069779374038\n",
      "Average character length of 'tc4': 102.36372950819673\n",
      "Average character length of 'tc5': 98.59127291505293\n",
      "Average character length of 'tc6': 94.35177968303455\n",
      "Average character length of 'tc7': 95.81495960385718\n",
      "Average character length of 'tc8': 97.39321148825066\n",
      "Average character length of 'tc9': 94.92190775681341\n"
     ]
    }
   ],
   "source": [
    "# Average character length of the text in the combined dataframe\n",
    "cols = ['headline'] + [f'tc{i}' for i in range(10)]\n",
    "for col in cols:\n",
    "    avg_length = df[col].str.len().mean()\n",
    "    print(f\"Average character length of '{col}': {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns are not of the correct data type\n",
    "df[cols] = df[cols].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay these are all reasonable character lengths, so I think the sentiment analyses should be much nicer compared to my previous attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Analysis\n",
    "In this section I'll use NLTK's sentiment analysis to convert all the columns into sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "julian assange free reach plea deal us wikileaks founder julian assange expect free prison agree plea deal us authority see sentence time already serve british prison assange plead guilty us charge one count conspiracy obtain disclose national defence information sky news uk report june\n"
     ]
    }
   ],
   "source": [
    "sample_headline = df['headline'].sample(1).values[0]\n",
    "print(sample_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.213, 'neu': 0.55, 'pos': 0.237, 'compound': -0.3818}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia.polarity_scores(sample_headline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compound Scores\n",
    "So this mechanism divides it's output into a negative, neutral, positive, and compound score.\n",
    "I'll just use compound for now, and see what the scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the sentiment analysis and create new columns for each score\n",
    "for col in cols:\n",
    "    df[f'nltk_{col}'] = df[col].apply(lambda x: pd.Series(sia.polarity_scores(x)['compound']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nltk_headline</th>\n",
       "      <th>nltk_tc0</th>\n",
       "      <th>nltk_tc1</th>\n",
       "      <th>nltk_tc2</th>\n",
       "      <th>nltk_tc3</th>\n",
       "      <th>nltk_tc4</th>\n",
       "      <th>nltk_tc5</th>\n",
       "      <th>nltk_tc6</th>\n",
       "      <th>nltk_tc7</th>\n",
       "      <th>nltk_tc8</th>\n",
       "      <th>nltk_tc9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "      <td>4800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.195921</td>\n",
       "      <td>0.104262</td>\n",
       "      <td>0.117701</td>\n",
       "      <td>0.115909</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>0.117196</td>\n",
       "      <td>0.112485</td>\n",
       "      <td>0.110992</td>\n",
       "      <td>0.110591</td>\n",
       "      <td>0.117524</td>\n",
       "      <td>0.106034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.504399</td>\n",
       "      <td>0.423808</td>\n",
       "      <td>0.423802</td>\n",
       "      <td>0.418869</td>\n",
       "      <td>0.405652</td>\n",
       "      <td>0.406945</td>\n",
       "      <td>0.407351</td>\n",
       "      <td>0.403429</td>\n",
       "      <td>0.407998</td>\n",
       "      <td>0.405584</td>\n",
       "      <td>0.402489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.987400</td>\n",
       "      <td>-0.984000</td>\n",
       "      <td>-0.999200</td>\n",
       "      <td>-0.975000</td>\n",
       "      <td>-0.987600</td>\n",
       "      <td>-0.989000</td>\n",
       "      <td>-0.991900</td>\n",
       "      <td>-0.965100</td>\n",
       "      <td>-0.970900</td>\n",
       "      <td>-0.978300</td>\n",
       "      <td>-0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.636900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.381800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.996100</td>\n",
       "      <td>0.990200</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>0.998300</td>\n",
       "      <td>0.996600</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>0.996500</td>\n",
       "      <td>0.993600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nltk_headline     nltk_tc0     nltk_tc1     nltk_tc2     nltk_tc3  \\\n",
       "count    4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean        0.195921     0.104262     0.117701     0.115909     0.118750   \n",
       "std         0.504399     0.423808     0.423802     0.418869     0.405652   \n",
       "min        -0.987400    -0.984000    -0.999200    -0.975000    -0.987600   \n",
       "25%         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%         0.025800     0.000000     0.000000     0.000000     0.000000   \n",
       "75%         0.636900     0.401900     0.421500     0.421500     0.421500   \n",
       "max         0.999900     0.998200     0.996100     0.990200     0.997800   \n",
       "\n",
       "          nltk_tc4     nltk_tc5     nltk_tc6     nltk_tc7     nltk_tc8  \\\n",
       "count  4800.000000  4800.000000  4800.000000  4800.000000  4800.000000   \n",
       "mean      0.117196     0.112485     0.110992     0.110591     0.117524   \n",
       "std       0.406945     0.407351     0.403429     0.407998     0.405584   \n",
       "min      -0.989000    -0.991900    -0.965100    -0.970900    -0.978300   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.401900     0.421500     0.401900     0.401900     0.401900   \n",
       "max       0.998300     0.996600     0.999400     0.995800     0.996500   \n",
       "\n",
       "          nltk_tc9  \n",
       "count  4800.000000  \n",
       "mean      0.106034  \n",
       "std       0.402489  \n",
       "min      -0.985600  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.381800  \n",
       "max       0.993600  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_cols = ['nltk_' + col for col in cols]\n",
    "df[nltk_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so it seems that a majority of scores are slightly positive. But this is across the columns. Perhaps after I run my special weighted mean on each post and then take the median grouped by month, I'll see a different story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean(row: pd.Series, cols: list[str]) -> float:\n",
    "    def w(i: int) -> float:\n",
    "        return (1/100) * (i - 10) ** 2\n",
    "\n",
    "    return sum([row[col] * w(i) for i, col in enumerate(cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "month\n",
       "Jan    0.607752\n",
       "Feb    0.576147\n",
       "Mar    0.470315\n",
       "Apr    0.482185\n",
       "May    0.600320\n",
       "Jun    0.528733\n",
       "Jul    0.514776\n",
       "Aug    0.432123\n",
       "Sep    0.464614\n",
       "Oct    0.493422\n",
       "Nov    0.566831\n",
       "Dec    0.480364\n",
       "Name: nltk_mu, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nltk_mu'] = df.apply(lambda row: weighted_mean(row, nltk_cols), axis=1)\n",
    "df.groupby('month', sort=False)['nltk_mu'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, still all positive! Note that this isn't a horrible thing, I shouldn't shape the results to my expectation. Looking at the actual S&P 500 graph:\n",
    "![S&P 500 Graph 2024](S&P_500_2024.png)\n",
    "\n",
    "Can see that there was a dip in March and July/August of that year, which does correspond to dips in sentiment scores around that time in the data, despite being positive, so perhaps there's something there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinBERT Sentiment Analysis\n",
    "Let's see how FinBERT does, given that it is specially trained on financial texts and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "finance_sentiment = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (26201 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (26201) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinance_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:159\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    126\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    158\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    161\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/pipelines/base.py:1368\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1361\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1362\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1365\u001b[39m         )\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/pipelines/base.py:1375\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1374\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/pipelines/base.py:1275\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1274\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:190\u001b[39m, in \u001b[36mTextClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters.keys():\n\u001b[32m    189\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1673\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1665\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1666\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1667\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1668\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1669\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1670\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1671\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1683\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1685\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1687\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1078\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m         token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1087\u001b[39m     attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/410-project/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:217\u001b[39m, in \u001b[36mBertEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.position_embedding_type == \u001b[33m\"\u001b[39m\u001b[33mabsolute\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    216\u001b[39m     position_embeddings = \u001b[38;5;28mself\u001b[39m.position_embeddings(position_ids)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[32m    218\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.LayerNorm(embeddings)\n\u001b[32m    219\u001b[39m embeddings = \u001b[38;5;28mself\u001b[39m.dropout(embeddings)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (26201) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "finance_sentiment(df_combined['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9233734607696533}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_sentiment(df_combined['text'].iloc[0][:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is currently an issue. The text is too large, and has too many characters for the BERT model to be able to process it.\n",
    "\n",
    "So I think the next step is to figure out how I want to chunk the data and combine the component scores into an aggregate score for each post, and then combine these post aggregates into a prediction for the month.\n",
    "I will have to be a bit careful about how I combine scores to achieve the effect I want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "410-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
